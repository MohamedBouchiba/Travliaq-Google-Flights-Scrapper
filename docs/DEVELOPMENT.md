# üë®‚Äçüíª Guide de D√©veloppement - Ajouter des Endpoints

Guide pour √©tendre l'API Travliaq avec de nouveaux endpoints de scraping.

---

## üéØ Objectif

Ajouter un nouvel endpoint pour scraper une autre partie de Google Flights (ex: d√©tails de vols, prix historiques, etc.)

---

## üìÅ Architecture du Projet
```
Travliaq-Google-Flights-Scrapper/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # ‚Üê Endpoints FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calendar_scraper.py     # ‚Üê Scraper calendrier
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ votre_nouveau_scraper.py # ‚Üê Nouveau scraper
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py              # ‚Üê Mod√®les Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ core/
‚îÇ       ‚îú‚îÄ‚îÄ driver_manager.py       # ‚Üê Gestion Chrome
‚îÇ       ‚îî‚îÄ‚îÄ scraper_pool.py         # ‚Üê Pool de subprocess
```

---

## üîß √âtape 1 : Cr√©er un Nouveau Scraper

### 1.1 Template de base

Cr√©er `src/scrapers/flight_details_scraper.py` :
```python
"""
Scraper pour r√©cup√©rer les d√©tails des vols
"""

import time
import random
from typing import Dict, List, Optional
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from ..core.driver_manager import DriverManager
from ..core.config import settings
from ..core.exceptions import ScraperException
from ..utils.logger import get_logger
from ..utils.validators import Validators

logger = get_logger(__name__)


class FlightDetailsScraper:
    """
    Scraper pour r√©cup√©rer les d√©tails des vols pour une date sp√©cifique
    """
    
    def __init__(self, headless: Optional[bool] = None):
        """Initialise le scraper"""
        self.driver_manager = DriverManager(headless=headless)
        self.driver = None
        self.wait = None
    
    def _build_url(self, origin: str, destination: str, date: str) -> str:
        """Construit l'URL Google Flights"""
        url = f"https://www.google.com/travel/flights"
        url += f"?q=Flights+from+{origin}+to+{destination}+on+{date}"
        url += "&curr=EUR&hl=fr"
        return url
    
    def _random_delay(self, min_sec: float = 2.0, max_sec: float = 5.0):
        """D√©lai al√©atoire"""
        time.sleep(random.uniform(min_sec, max_sec))
    
    def scrape(
        self,
        origin: str,
        destination: str,
        date: str
    ) -> List[Dict]:
        """
        Scrape les d√©tails des vols pour une date
        
        Args:
            origin: Code IATA d√©part
            destination: Code IATA arriv√©e
            date: Date (YYYY-MM-DD)
            
        Returns:
            Liste de vols avec d√©tails
        """
        # Validation
        origin, destination = Validators.validate_route(origin, destination)
        Validators.validate_date(date)
        
        logger.info(f"üï∑Ô∏è Scraping flights: {origin}->{destination} on {date}")
        
        flights = []
        
        try:
            # Initialiser le driver
            self.driver = self.driver_manager.create_driver()
            self.wait = self.driver_manager.wait
            
            # Charger la page
            url = self._build_url(origin, destination, date)
            self.driver.get(url)
            time.sleep(5)
            
            # TODO: Impl√©menter la logique de scraping
            # 1. Attendre que les r√©sultats se chargent
            # 2. Extraire les informations de chaque vol
            # 3. Parser et structurer les donn√©es
            
            # Exemple de structure de retour
            flights = [
                {
                    "airline": "Brussels Airlines",
                    "flight_number": "SN3175",
                    "departure_time": "10:30",
                    "arrival_time": "11:45",
                    "duration": "1h 15m",
                    "stops": 0,
                    "price": 149.00,
                    "aircraft": "A320",
                    "cabin_class": "Economy"
                }
            ]
            
            logger.info(f"‚úÖ {len(flights)} vols trouv√©s")
            return flights
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping: {e}", exc_info=True)
            raise ScraperException(f"Erreur lors du scraping: {e}")
        finally:
            self.close()
    
    def close(self):
        """Ferme le driver"""
        if self.driver_manager:
            self.driver_manager.close()
```

### 1.2 Conseils pour le scraping

**Identifier les s√©lecteurs CSS/XPath** :

1. Ouvrir Google Flights dans Chrome
2. F12 ‚Üí Inspecter les √©l√©ments
3. Trouver les s√©lecteurs uniques
4. Tester dans la console :
```javascript
   document.querySelectorAll('[data-test-id="flight-card"]')
```

**Attendre le chargement** :
```python
# Attendre un √©l√©ment sp√©cifique
flight_cards = self.wait.until(
    EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".flight-card"))
)
```

**Extraire les donn√©es** :
```python
for card in flight_cards:
    try:
        airline = card.find_element(By.CSS_SELECTOR, ".airline-name").text
        price_text = card.find_element(By.CSS_SELECTOR, ".price").text
        price = float(re.sub(r'[^\d.]', '', price_text))
        
        flights.append({
            "airline": airline,
            "price": price,
            # ... autres champs
        })
    except Exception as e:
        logger.warning(f"Erreur extraction vol: {e}")
        continue
```

---

## üìä √âtape 2 : Cr√©er les Mod√®les Pydantic

### 2.1 Ajouter dans `src/models/schemas.py`
```python
# Request model
class FlightDetailsRequest(BaseModel):
    """Requ√™te pour obtenir les d√©tails des vols"""
    origin: str = Field(..., description="Code IATA d√©part", example="BRU")
    destination: str = Field(..., description="Code IATA arriv√©e", example="CDG")
    date: str = Field(..., description="Date (YYYY-MM-DD)", example="2025-11-15")
    force_refresh: bool = Field(default=False, description="Forcer le re-scraping")
    
    @validator('origin', 'destination')
    def validate_airport_codes(cls, v):
        return Validators.validate_airport_code(v)
    
    @validator('date')
    def validate_date(cls, v):
        Validators.validate_date(v)
        return v


# Response models
class FlightDetail(BaseModel):
    """D√©tails d'un vol"""
    airline: str
    flight_number: Optional[str] = None
    departure_time: str
    arrival_time: str
    duration: str
    stops: int
    price: float
    aircraft: Optional[str] = None
    cabin_class: Optional[str] = None


class FlightDetailsResponse(BaseModel):
    """R√©ponse avec les d√©tails des vols"""
    origin: str
    destination: str
    date: str
    flights: List[FlightDetail]
    total_flights: int
    min_price: Optional[float] = None
    max_price: Optional[float] = None
    scraped_at: datetime = Field(default_factory=datetime.now)
    from_cache: bool = False
    
    @classmethod
    def from_flights_list(
        cls,
        origin: str,
        destination: str,
        date: str,
        flights: List[Dict],
        from_cache: bool = False
    ):
        """Factory pour cr√©er la r√©ponse"""
        if not flights:
            return cls(
                origin=origin,
                destination=destination,
                date=date,
                flights=[],
                total_flights=0,
                from_cache=from_cache
            )
        
        prices = [f["price"] for f in flights if "price" in f]
        
        return cls(
            origin=origin,
            destination=destination,
            date=date,
            flights=[FlightDetail(**f) for f in flights],
            total_flights=len(flights),
            min_price=min(prices) if prices else None,
            max_price=max(prices) if prices else None,
            from_cache=from_cache
        )
```

---

## üåê √âtape 3 : Ajouter l'Endpoint dans l'API

### 3.1 Ajouter dans `src/api/main.py`
```python
# En haut du fichier, ajouter l'import
from ..scrapers.flight_details_scraper import FlightDetailsScraper
from ..models.schemas import FlightDetailsRequest, FlightDetailsResponse

# Ajouter l'endpoint
@app.get(
    f"{API_PREFIX}/flight-details",
    response_model=FlightDetailsResponse,
    tags=["Scraping"],
    summary="R√©cup√®re les d√©tails des vols",
    description="R√©cup√®re tous les vols disponibles pour une date sp√©cifique"
)
async def get_flight_details(
    origin: str = Query(..., description="Code IATA d√©part"),
    destination: str = Query(..., description="Code IATA arriv√©e"),
    date: str = Query(..., description="Date (YYYY-MM-DD)"),
    force_refresh: bool = Query(False, description="Forcer le re-scraping"),
):
    """Endpoint pour r√©cup√©rer les d√©tails des vols"""
    import asyncio
    from concurrent.futures import ThreadPoolExecutor
    
    start_time = time.time()
    
    logger.info(f"üì• Requ√™te flight-details: {origin}->{destination} on {date}")
    
    # Normaliser
    origin = origin.upper()
    destination = destination.upper()
    
    # TODO: V√©rifier le cache si impl√©ment√©
    # if not force_refresh:
    #     cached = db_manager.get_cached_flights(origin, destination, date)
    #     if cached:
    #         return FlightDetailsResponse.from_flights_list(...)
    
    # Scraping
    try:
        logger.info(f"üï∑Ô∏è  Lancement scraping...")
        
        # Cr√©er une fonction wrapper pour le subprocess
        def scrape_wrapper():
            scraper = FlightDetailsScraper(headless=settings.headless)
            return scraper.scrape(origin, destination, date)
        
        # Ex√©cuter de mani√®re asynchrone
        loop = asyncio.get_event_loop()
        executor = ThreadPoolExecutor()
        
        flights = await loop.run_in_executor(
            executor,
            scrape_wrapper
        )
        
        if not flights:
            raise HTTPException(
                status_code=404,
                detail="Aucun vol trouv√©"
            )
        
        # TODO: Sauvegarder en cache
        # db_manager.save_flights(origin, destination, date, flights)
        
        duration = time.time() - start_time
        logger.info(f"‚úì Scraping termin√© ({duration:.1f}s)")
        
        return FlightDetailsResponse.from_flights_list(
            origin=origin,
            destination=destination,
            date=date,
            flights=flights,
            from_cache=False
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        if SENTRY_AVAILABLE and settings.sentry_dsn and sentry_sdk:
            sentry_sdk.capture_exception(e)
        raise HTTPException(status_code=500, detail=str(e))
```

---

## üß™ √âtape 4 : Tester le Nouvel Endpoint

### 4.1 Script de test

Cr√©er `tests/test_flight_details.py` :
```python
"""
Test du nouvel endpoint flight-details
"""

import requests
import time

BASE_URL = "http://localhost:8000/api/v1"


def test_flight_details():
    """Test du scraping de d√©tails de vols"""
    print("\n" + "="*70)
    print("üß™ TEST FLIGHT DETAILS")
    print("="*70)
    
    params = {
        "origin": "BRU",
        "destination": "CDG",
        "date": "2025-11-15",
        "force_refresh": True
    }
    
    print(f"\nüìç Route: {params['origin']} ‚Üí {params['destination']}")
    print(f"üìÖ Date: {params['date']}")
    print("\n‚è≥ Scraping en cours...\n")
    
    start = time.time()
    
    try:
        response = requests.get(
            f"{BASE_URL}/flight-details",
            params=params,
            timeout=120
        )
        
        duration = time.time() - start
        
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ Succ√®s en {duration:.1f}s")
            print(f"\nüìä R√©sultats:")
            print(f"   Vols trouv√©s: {data['total_flights']}")
            
            if data['min_price']:
                print(f"   Prix min: {data['min_price']}‚Ç¨")
            if data['max_price']:
                print(f"   Prix max: {data['max_price']}‚Ç¨")
            
            print(f"\n‚úàÔ∏è  D√©tails des vols:")
            for flight in data['flights'][:5]:  # Top 5
                print(f"\n   {flight['airline']}")
                print(f"      D√©part: {flight['departure_time']} ‚Üí Arriv√©e: {flight['arrival_time']}")
                print(f"      Dur√©e: {flight['duration']} | Escales: {flight['stops']}")
                print(f"      Prix: {flight['price']}‚Ç¨")
        else:
            print(f"‚ùå Erreur {response.status_code}")
            print(f"   {response.text}")
            
    except requests.Timeout:
        print("‚è∞ Timeout!")
    except Exception as e:
        print(f"‚ùå Erreur: {e}")


if __name__ == "__main__":
    test_flight_details()
    input("\n‚è∏Ô∏è  Appuyez sur ENTR√âE...")
```

### 4.2 Tester
```bash
# Terminal 1: Lancer l'API
python scripts/run_api.py

# Terminal 2: Tester
python tests/test_flight_details.py
```

---

## üìù √âtape 5 : Documenter

### 5.1 Mettre √† jour le README
```markdown
## Nouveaux Endpoints

### GET /api/v1/flight-details

R√©cup√®re les d√©tails des vols pour une date sp√©cifique.

**Param√®tres** :
- `origin` (string) : Code IATA d√©part
- `destination` (string) : Code IATA arriv√©e
- `date` (string) : Date au format YYYY-MM-DD
- `force_refresh` (bool) : Forcer le re-scraping

**Exemple** :
\`\`\`bash
curl "http://localhost:8000/api/v1/flight-details?origin=BRU&destination=CDG&date=2025-11-15"
\`\`\`
```

---

## ‚úÖ Checklist pour Ajouter un Endpoint

- [ ] Cr√©er le scraper dans `src/scrapers/`
- [ ] Ajouter les mod√®les Pydantic dans `src/models/schemas.py`
- [ ] Ajouter l'endpoint dans `src/api/main.py`
- [ ] Impl√©menter le cache (optionnel)
- [ ] Cr√©er un script de test dans `tests/`
- [ ] Tester en local
- [ ] Documenter dans README.md
- [ ] Commit et push

---

## üéØ Bonnes Pratiques

### 1. Anti-d√©tection
```python
# Toujours utiliser des d√©lais al√©atoires
self._random_delay(2, 5)

# Simuler un comportement humain
self.driver_manager.simulate_human_behavior()

# Varier les user agents
# (d√©j√† g√©r√© par DriverManager)
```

### 2. Gestion d'erreurs
```python
try:
    element = self.wait.until(
        EC.presence_of_element_located((By.CSS_SELECTOR, ".selector"))
    )
except TimeoutException:
    logger.warning("√âl√©ment non trouv√©")
    # Fallback ou skip
```

### 3. Logging appropri√©
```python
logger.info("Action importante")     # Info g√©n√©rale
logger.debug("D√©tail technique")    # Debug seulement
logger.warning("Probl√®me mineur")   # Attention
logger.error("Erreur critique")     # Erreur
```

### 4. Validation des inputs
```python
# Toujours valider avec les Validators
origin, destination = Validators.validate_route(origin, destination)
date = Validators.validate_date(date)
```

---

## üÜò Aide au Debugging

### Scraper ne trouve pas les √©l√©ments
```python
# Sauvegarder une capture d'√©cran
self.driver.save_screenshot("debug.png")

# Logger le HTML
html = self.driver.page_source
logger.debug(f"HTML: {html[:500]}")

# Tester en mode non-headless
# Mettre HEADLESS=false dans .env
```

### Performances lentes
```python
# D√©sactiver le chargement des images
options.add_experimental_option('prefs', {
    'profile.managed_default_content_settings.images': 2
})

# Utiliser des attentes explicites au lieu de time.sleep()
self.wait.until(EC.presence_of_element_located(...))
```

---

## üìö Ressources

- [Selenium Documentation](https://www.selenium.dev/documentation/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [Chrome DevTools](https://developer.chrome.com/docs/devtools/)

---

**Bon d√©veloppement ! üöÄ**