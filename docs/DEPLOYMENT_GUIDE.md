# üöÄ Guide de D√©ploiement - Nouveau Calendar Scraper

## üìã Pr√©-requis

### Syst√®me
- Python 3.9+
- Chrome/Chromium install√©
- ChromeDriver compatible (fourni dans `drivers/`)

### D√©pendances
Toutes les d√©pendances existantes sont conserv√©es. Aucune nouvelle installation n√©cessaire.

```bash
# V√©rifier les d√©pendances
pip install -r requirements.txt
```

## üîß Installation

### Option 1: Remplacement Direct (Recommand√©)

```bash
# 1. Sauvegarder l'ancien scraper (optionnel)
cp src/scrapers/calendar_scraper.py src/scrapers/calendar_scraper.py.backup

# 2. Copier le nouveau scraper
cp calendar_scraper.py src/scrapers/calendar_scraper.py

# 3. V√©rifier que tout fonctionne
python test_new_scraper.py
```

### Option 2: D√©ploiement avec Git

```bash
# 1. Cr√©er une nouvelle branche
git checkout -b feature/new-calendar-scraper

# 2. Ajouter les fichiers
git add src/scrapers/calendar_scraper.py
git add test_new_scraper.py
git add test_api_endpoint.py
git add README_INTEGRATION.md

# 3. Commit
git commit -m "feat: Nouveau calendar scraper avec navigation intelligente"

# 4. Pousser et cr√©er une PR
git push origin feature/new-calendar-scraper
```

## üß™ Tests

### 1. Tests Unitaires

```bash
# Tester le scraper seul
python test_new_scraper.py
```

**Dur√©e estim√©e:** 5-10 minutes  
**Tests effectu√©s:**
- Scraping basique
- Int√©gration DB
- Multi-mois
- Gestion d'erreurs
- Compatibilit√© API

### 2. Tests d'Int√©gration API

```bash
# 1. Lancer l'API
python -m uvicorn src.api.main:app --reload

# 2. Dans un autre terminal, tester l'endpoint
python test_api_endpoint.py
```

**Dur√©e estim√©e:** 3-5 minutes  
**Tests effectu√©s:**
- Health check
- Nouveau scraping
- Lecture cache
- Stats cache
- Multi-routes
- Cas d'erreur

### 3. Test Manuel

```bash
# Mode visible (debug)
python -c "
from src.scrapers.calendar_scraper import CalendarScraper
scraper = CalendarScraper(headless=False)
prices = scraper.scrape('BRU', 'CDG', 3)
print(f'{len(prices)} prix r√©cup√©r√©s')
"
```

## üîÑ Migration

### √âtape 1: Backup de la Base de Donn√©es

```bash
# SQLite
cp data/flights.db data/flights.db.backup

# PostgreSQL (si applicable)
pg_dump -U user -d flights_db > backup.sql
```

### √âtape 2: Nettoyer le Cache (Optionnel)

```bash
# Via l'API
curl -X DELETE "http://localhost:8000/api/v1/cache/clear?days=7"

# Ou via Python
python -c "
from src.database.manager import db_manager
db_manager.clear_old_cache(days=7)
print('Cache nettoy√©')
"
```

### √âtape 3: Test de R√©gression

```bash
# Comparer ancien vs nouveau scraper
# (N√©cessite de garder l'ancien fichier)

python -c "
# Test avec ancien scraper
from src.scrapers.calendar_scraper_backup import CalendarScraper as OldScraper
old = OldScraper()
old_prices = old.scrape('BRU', 'CDG', 2)

# Test avec nouveau scraper  
from src.scrapers.calendar_scraper import CalendarScraper
new = CalendarScraper()
new_prices = new.scrape('BRU', 'CDG', 2)

print(f'Ancien: {len(old_prices)} prix')
print(f'Nouveau: {len(new_prices)} prix')
print(f'Diff√©rence: {len(new_prices) - len(old_prices)} prix')
"
```

## üåê D√©ploiement Production

### Configuration Production

**1. Variables d'Environnement**

Cr√©er/modifier `.env.production`:

```env
# Environment
ENVIRONMENT=production
DEBUG=false

# API
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=false

# Scraper
HEADLESS=true
SCREENSHOT_ON_ERROR=false  # D√©sactiver en prod pour √©conomiser espace
TIMEOUT=30
MAX_RETRIES=3

# Database
DATABASE_URL=postgresql://user:pass@localhost/flights_db  # ou SQLite

# Cache
CACHE_TTL_MINUTES=120  # 2h en production

# Logs
LOG_LEVEL=INFO
LOG_FILE=logs/production.log

# Rate Limiting
REQUESTS_PER_HOUR=30
DELAY_BETWEEN_REQUESTS_MIN=3.0
DELAY_BETWEEN_REQUESTS_MAX=7.0

# Proxy (optionnel)
USE_PROXY=false
PROXY_URL=
```

**2. Lancer en Production**

```bash
# Avec Uvicorn (d√©veloppement/staging)
uvicorn src.api.main:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 2 \
  --env-file .env.production

# Avec Gunicorn (production)
gunicorn src.api.main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --timeout 180 \
  --access-logfile logs/access.log \
  --error-logfile logs/error.log
```

### Docker Deployment

**Dockerfile** (√† cr√©er si n√©cessaire):

```dockerfile
FROM python:3.11-slim

# Installer Chrome et ChromeDriver
RUN apt-get update && apt-get install -y \
    chromium \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copier requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code
COPY . .

# Cr√©er les r√©pertoires
RUN mkdir -p data logs screenshots

# Variables d'environnement
ENV HEADLESS=true
ENV CHROME_BIN=/usr/bin/chromium
ENV CHROMEDRIVER_PATH=/usr/bin/chromedriver

EXPOSE 8000

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml**:

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env.production
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./screenshots:/app/screenshots
    restart: unless-stopped
    
  # PostgreSQL (optionnel)
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: flights_db
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  postgres_data:
```

**D√©ploiement:**

```bash
# Build
docker-compose build

# Start
docker-compose up -d

# Logs
docker-compose logs -f api

# Stop
docker-compose down
```

## üìä Monitoring

### 1. Logs

```bash
# Logs en temps r√©el
tail -f logs/production.log

# Erreurs uniquement
tail -f logs/errors.log

# Rechercher des erreurs
grep "ERROR" logs/production.log | tail -20
```

### 2. Health Check

```bash
# V√©rifier que l'API r√©pond
curl http://localhost:8000/api/v1/health

# Avec watch (toutes les 10s)
watch -n 10 'curl -s http://localhost:8000/api/v1/health | jq'
```

### 3. M√©triques

```bash
# Stats du cache
curl http://localhost:8000/api/v1/cache/stats | jq

# Tester une route
curl "http://localhost:8000/api/v1/calendar-prices?origin=BRU&destination=CDG&months=2"
```

### 4. Alertes (Optionnel)

Configurer des alertes avec un service externe:

```python
# exemple_alert.py
import requests
import time

def check_health():
    try:
        r = requests.get('http://localhost:8000/api/v1/health', timeout=5)
        if r.status_code != 200:
            send_alert(f"API unhealthy: {r.status_code}")
    except Exception as e:
        send_alert(f"API down: {e}")

def send_alert(message):
    # Slack, email, SMS, etc.
    pass

if __name__ == "__main__":
    while True:
        check_health()
        time.sleep(300)  # Check every 5 minutes
```

## üîÑ Rollback

En cas de probl√®me:

### Rollback Rapide

```bash
# 1. Restaurer l'ancien scraper
cp src/scrapers/calendar_scraper.py.backup src/scrapers/calendar_scraper.py

# 2. Red√©marrer l'API
# Avec systemd
sudo systemctl restart flights-api

# Avec Docker
docker-compose restart api

# Manuel
pkill -f uvicorn && uvicorn src.api.main:app
```

### Rollback Git

```bash
# Trouver le commit
git log --oneline | grep calendar

# Revert
git revert <commit-hash>

# Ou reset (attention!)
git reset --hard HEAD~1
```

## üìù Checklist de D√©ploiement

- [ ] Backup de la base de donn√©es effectu√©
- [ ] Tests unitaires pass√©s (test_new_scraper.py)
- [ ] Tests API pass√©s (test_api_endpoint.py)
- [ ] Configuration production v√©rifi√©e (.env.production)
- [ ] Logs configur√©s correctement
- [ ] Screenshots d√©sactiv√©s en production
- [ ] Health check fonctionnel
- [ ] Cache nettoy√© si n√©cessaire
- [ ] Documentation mise √† jour
- [ ] Rollback plan ready
- [ ] Monitoring configur√©
- [ ] √âquipe notifi√©e du d√©ploiement

## üÜò Troubleshooting

### Probl√®me: "ChromeDriver introuvable"

```bash
# V√©rifier ChromeDriver
ls -la drivers/chromedriver.exe

# T√©l√©charger si n√©cessaire
# https://chromedriver.chromium.org/downloads
```

### Probl√®me: "Impossible d'ouvrir le calendrier"

```bash
# Tester en mode visible
python -c "
from src.scrapers.calendar_scraper import CalendarScraper
scraper = CalendarScraper(headless=False)
# Observer ce qui se passe
"
```

### Probl√®me: "Timeout constant"

```env
# Augmenter les timeouts dans .env
TIMEOUT=45
DELAY_BETWEEN_REQUESTS_MAX=10.0
```

### Probl√®me: "Rate limit Google"

```env
# R√©duire la fr√©quence
REQUESTS_PER_HOUR=20
DELAY_BETWEEN_REQUESTS_MIN=5.0
DELAY_BETWEEN_REQUESTS_MAX=10.0

# Activer un proxy
USE_PROXY=true
PROXY_URL=http://proxy:port
```

## üìû Support

- **Documentation:** README_INTEGRATION.md
- **Tests:** test_new_scraper.py, test_api_endpoint.py
- **Logs:** logs/production.log, logs/errors.log
- **Screenshots:** screenshots/ (si activ√©)

---

**Version:** 2.0.0  
**Date:** Octobre 2025  
**Status:** ‚úÖ Production Ready
